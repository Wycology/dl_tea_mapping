{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/Ch7mqEY7XcUHiMBVkt1m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a4454b8d1bf74dd591794ea4d1269418": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e41954eec521447dbc7adbdfab3f8ff5",
              "IPY_MODEL_fe6516ed40044d8486216556b8198e5f",
              "IPY_MODEL_2ea8f9630f10408dbffd4ebfe4080367"
            ],
            "layout": "IPY_MODEL_aeecc78b850840019ecdd165bde8bbdc"
          }
        },
        "e41954eec521447dbc7adbdfab3f8ff5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f118fe47e8434475849c872c9c32454f",
            "placeholder": "​",
            "style": "IPY_MODEL_c15dd268aa174adca0442670a1504213",
            "value": "model.safetensors: 100%"
          }
        },
        "fe6516ed40044d8486216556b8198e5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad5c5a30e53d48d4b068e107e0b8856e",
            "max": 46807446,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_666a4c71b5984f26bf410b4f484a2d61",
            "value": 46807446
          }
        },
        "2ea8f9630f10408dbffd4ebfe4080367": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_495dd18131974e149e875e051b816e3f",
            "placeholder": "​",
            "style": "IPY_MODEL_5dcb375a56c4466dac8c20de759de002",
            "value": " 46.8M/46.8M [00:00&lt;00:00, 90.0MB/s]"
          }
        },
        "aeecc78b850840019ecdd165bde8bbdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f118fe47e8434475849c872c9c32454f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c15dd268aa174adca0442670a1504213": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad5c5a30e53d48d4b068e107e0b8856e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "666a4c71b5984f26bf410b4f484a2d61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "495dd18131974e149e875e051b816e3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dcb375a56c4466dac8c20de759de002": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wycology/dl_tea_mapping/blob/main/dl_tea_mapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color='green'><b> SATELLITE DATA FOR AGRICULTURAL ECONOMISTS</b></font>\n",
        "\n",
        "\n",
        "<font color='blue'><b>THEORY AND PRACTICE</b></font>\n",
        "\n",
        "**Mapping tea plantations in Central Kenya: _Deep Learning Approach_**\n",
        "\n",
        "\n",
        "*David Wuepper, Lisa Biber-Freudenberger, Hadi, Wyclife Agumba Oluoch*\n",
        "\n",
        "[Land Economics Group](https://www.ilr1.uni-bonn.de/en/research/research-groups/land-economics), University of Bonn, Bonn, Germany\n",
        "\n",
        "---\n",
        "\n",
        "# **Background**\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "hOhakLtRfInx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, we introduce basics of using deep learning approach to segment tea fields with a practical example at the foot of Mount Kenya. We obtained high resolution Sentinel-2 satellite image from [Google Earth Engine](https://code.earthengine.google.com/60cf3e783458009bd8378eaded30f5c7). On the other hand, we obtained labels by manually digitizing tea plantations within QGIS using Google Satellite Hybrid basemap. The labels cover a small portion of the downloaded Satellite image so that we can train the model and use it to segment tea fields elsewhere.\n",
        "We used torchgeo for this modeling task due to the following reasons:\n",
        "1. It is simple to use, eliminating a lot of issues such as georeferencing, chipping, label creation.\n",
        "2. It also maximally obtain training samples from the region of interest."
      ],
      "metadata": {
        "id": "HjuDkTPdoG63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading libraries\n",
        "---\n",
        "Since `torchgeo` is not natively installed in colab, we will have to install it. We will also install `torchseg` to help with the segmentation work. Other supporting libraries will just be imported as they are already pre-installed in colab.\n"
      ],
      "metadata": {
        "id": "4iju3k1hqpBU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgE0FtVBfGTS"
      },
      "outputs": [],
      "source": [
        "# Install libraries not already available in colab\n",
        "!pip install torchgeo\n",
        "!pip install torchseg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import json\n",
        "import torch\n",
        "import rasterio\n",
        "import torchseg\n",
        "import torchgeo\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torchgeo.transforms import AppendNDVI\n",
        "from rasterio.transform import from_bounds\n",
        "from torchgeo.samplers import RandomGeoSampler, GridGeoSampler\n",
        "from torchgeo.datasets import VectorDataset, RasterDataset, stack_samples"
      ],
      "metadata": {
        "id": "OH-_FMZYgRbB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After installing the libraries, we make an important step of confirming the working directory. This is important since both our image and gpkg will be read from this location so we need to be sure of the path. We can use `pwd` function to print it."
      ],
      "metadata": {
        "id": "PX1Es_PIrSAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pwd # Confirm the working directory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0dSokz-lgRXe",
        "outputId": "972e49a2-06d5-48dd-d20f-69c7ce9694b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the Dataset\n",
        "---\n",
        "With `torchgeo`, we do not have to pre-chip the satellite image into small chips of say 256 by 256 pixels. This it achieves on the fly. However, we need to tell it the path to where our satellite image is. In fact, we can have several large images here. For now, it is the only _.tif_ image our working directory, wo we define the class as follows:"
      ],
      "metadata": {
        "id": "GuTbGa3zrr-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the GeoTiff dataset class\n",
        "\n",
        "class GeoTiffDataset(RasterDataset):\n",
        "  filename_glob = \"*.tif\"\n",
        "raster_data = GeoTiffDataset(paths = \"/content\")"
      ],
      "metadata": {
        "id": "5s-0_FwVgRUz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In that case, raster_data is a blueprint of the satellite image we have in the directory. Next, we do the same for the label data. This label data is a _.gpkg_ file which has a column stating the identity of the each polygon as either tea or not tea. In other words, the class column. Here, we call the class column as `tea_no_tea`. This is very important as it is what the library uses to create a label binary layer under the hood to intersect with the satellite image. We achieve this as follows:"
      ],
      "metadata": {
        "id": "UHfwlM8YsbJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the label (here vector but can also be mask raster) dataset class.\n",
        "# Remember to include label name. Never forget this!\n",
        "\n",
        "class LabelDataset(VectorDataset):\n",
        "  filename_glob = \"*.gpkg\"\n",
        "label_data = LabelDataset(paths = \"/content\", label_name = \"tea_no_tea\")"
      ],
      "metadata": {
        "id": "z8R7eOUbgQ5J"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combining raster_data and label_data\n",
        "\n",
        "Now that we have blueprints of both raster_data and label_data, the next step is to intersect the two. This will behave like _cropping_ or _clipping_ the raster to the extent of the label_data. This is the reason why we are not worried that the raster extent is bigger than the label extent. Chips for training the model will only be ontained from where the two datasets intersect/overlap. Regions outside the label_data will not be sampled. As simple as it can get, we achieve this intersection using an _&_ operator."
      ],
      "metadata": {
        "id": "I_c4iF9GtXtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the intersection of the raster and vector/label datasets\n",
        "\n",
        "training_data = raster_data & label_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SblN_Gjg3eL",
        "outputId": "a6c4b849-fade-4316-bb76-08a15e09248c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting LabelDataset res from (0.0001, 0.0001) to (10.0, 10.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You notice the printout that **Converting LabelDataset res from (0.0001, 0.0001) to (10.0, 10.0)**. This tells us that our vector label data with polygons has now been converted to a binary raster under the hood with a pixel size similar to that of the satellite image we have - Sentinel-2. Something imortant to note also is that the pixels in both layers has been **aligned**."
      ],
      "metadata": {
        "id": "ixDgPJdKuXB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Append variables derivable from the bands, such as NDVI\n",
        "\n",
        "append_ndvi = AppendNDVI(index_nir = 7, index_red = 3)"
      ],
      "metadata": {
        "id": "AA-Z6zW-g3ag"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sampler that will execute the task of extracting labels\n",
        "\n",
        "sampler = RandomGeoSampler(dataset = training_data, size = 32, length = 1000)"
      ],
      "metadata": {
        "id": "PLvw4xWtg3Yr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a customized collate function to append NDVI to the sampled images\n",
        "def custom_collate_fn(samples):\n",
        "  for sample in samples:\n",
        "    sample[\"image\"] = append_ndvi(sample[\"image\"])[0]\n",
        "  return stack_samples(samples)"
      ],
      "metadata": {
        "id": "gR9-KH-lg3Wb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the dataloader. This is the function that will be serving the role of availing batches of extracted samples for model training\n",
        "dataloader = DataLoader(\n",
        "    dataset = training_data,\n",
        "    batch_size = 50,\n",
        "    sampler = sampler,\n",
        "    collate_fn = custom_collate_fn # normally would be stack_samples\n",
        ")"
      ],
      "metadata": {
        "id": "bRHN5D4Qg3UF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the dataloader to confirm that it is able to load the data\n",
        "\n",
        "for batch in dataloader:\n",
        "  image = batch[\"image\"][:, :12, :, :]\n",
        "  mask = image[:, -1, :, :]\n",
        "\n",
        "  print(f\"Image batch length: {len(image)}\")\n",
        "  print(f\"Mask batch length: {len(mask)}\")\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WS4bBubNhfRD",
        "outputId": "35562901-566a-4518-e8be-13b2aa64ec50"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image batch length: 50\n",
            "Mask batch length: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We will use Unet model from torchseg. Which is pretrained so we do not need to build it from scratch\n",
        "model = torchseg.Unet(\n",
        "    encoder_name = \"resnet18\",\n",
        "    encoder_weights = False,\n",
        "    in_channels = 11,\n",
        "    classes = 2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "a4454b8d1bf74dd591794ea4d1269418",
            "e41954eec521447dbc7adbdfab3f8ff5",
            "fe6516ed40044d8486216556b8198e5f",
            "2ea8f9630f10408dbffd4ebfe4080367",
            "aeecc78b850840019ecdd165bde8bbdc",
            "f118fe47e8434475849c872c9c32454f",
            "c15dd268aa174adca0442670a1504213",
            "ad5c5a30e53d48d4b068e107e0b8856e",
            "666a4c71b5984f26bf410b4f484a2d61",
            "495dd18131974e149e875e051b816e3f",
            "5dcb375a56c4466dac8c20de759de002"
          ]
        },
        "id": "Nw2BEUPKhfN0",
        "outputId": "686b87dd-e449-4f8d-d2dd-42cc5a904131"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/46.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4454b8d1bf74dd591794ea4d1269418"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use cuda if available, otherwise cpu\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "EU-b0DRbhfLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function and optimizer\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = -1)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
      ],
      "metadata": {
        "id": "mGgJMtephfJR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "metrics = {\"loss\": [], \"accuracy\": []}\n",
        "num_epochs = 20"
      ],
      "metadata": {
        "id": "s17JZNznhfGz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  epoch_loss = 0.0\n",
        "  total_correct = 0\n",
        "  total_pixels = 0\n",
        "\n",
        "  with tqdm(dataloader, desc = f\"Epoch {epoch + 1} / {num_epochs}\") as pbar:\n",
        "    for batch in pbar:\n",
        "      images = batch[\"image\"][:, :12, :, :].to(device)\n",
        "      masks = batch[\"mask\"].to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, masks.long())\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "      # Backpropagation\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Calculate accuracy\n",
        "      preds = torch.argmax(outputs, dim = 1)\n",
        "      total_correct += (preds == masks).sum().item()\n",
        "      total_pixels += masks.numel()\n",
        "\n",
        "      pbar.set_postfix(loss = loss.item())\n",
        "\n",
        "  epoch_accuracy = total_correct / total_pixels * 100\n",
        "  metrics[\"loss\"].append(epoch_loss)\n",
        "  metrics[\"accuracy\"].append(epoch_accuracy)\n",
        "\n",
        "  print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "  # Save training metrics\n",
        "  with open(\"/content/training_metrics.json\", \"w\") as f:\n",
        "    json.dump(metrics, f)\n",
        "\n",
        "  print(\"Training metrics saved to '/content/training_metrics.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PO13w41IhfEb",
        "outputId": "6eda3ad4-a0e6-4476-f3b7-c08baac79021"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 / 20: 100%|██████████| 20/20 [02:01<00:00,  6.09s/it, loss=0.254]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 7.5190, Accuracy: 83.15%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 / 20: 100%|██████████| 20/20 [01:59<00:00,  5.96s/it, loss=0.189]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/20], Loss: 4.2078, Accuracy: 91.13%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 / 20: 100%|██████████| 20/20 [02:02<00:00,  6.12s/it, loss=0.148]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/20], Loss: 3.3024, Accuracy: 93.00%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 / 20: 100%|██████████| 20/20 [01:59<00:00,  5.97s/it, loss=0.121]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/20], Loss: 2.7195, Accuracy: 94.30%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 / 20: 100%|██████████| 20/20 [02:00<00:00,  6.04s/it, loss=0.104]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/20], Loss: 2.2535, Accuracy: 95.38%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 / 20: 100%|██████████| 20/20 [01:58<00:00,  5.93s/it, loss=0.0906]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/20], Loss: 1.9878, Accuracy: 95.92%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 / 20: 100%|██████████| 20/20 [02:05<00:00,  6.29s/it, loss=0.0783]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/20], Loss: 1.7349, Accuracy: 96.50%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 / 20: 100%|██████████| 20/20 [02:08<00:00,  6.45s/it, loss=0.0797]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/20], Loss: 1.5612, Accuracy: 96.88%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 / 20: 100%|██████████| 20/20 [02:08<00:00,  6.42s/it, loss=0.0638]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/20], Loss: 1.3696, Accuracy: 97.36%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 / 20: 100%|██████████| 20/20 [02:09<00:00,  6.50s/it, loss=0.0578]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/20], Loss: 1.2177, Accuracy: 97.70%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11 / 20: 100%|██████████| 20/20 [02:06<00:00,  6.33s/it, loss=0.0563]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/20], Loss: 1.0954, Accuracy: 97.99%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12 / 20: 100%|██████████| 20/20 [02:07<00:00,  6.39s/it, loss=0.0516]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/20], Loss: 0.9898, Accuracy: 98.24%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13 / 20: 100%|██████████| 20/20 [02:04<00:00,  6.21s/it, loss=0.04]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/20], Loss: 0.9191, Accuracy: 98.36%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14 / 20: 100%|██████████| 20/20 [02:12<00:00,  6.61s/it, loss=0.0377]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/20], Loss: 0.7689, Accuracy: 98.74%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15 / 20: 100%|██████████| 20/20 [02:09<00:00,  6.45s/it, loss=0.0386]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/20], Loss: 0.7189, Accuracy: 98.82%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16 / 20: 100%|██████████| 20/20 [02:14<00:00,  6.73s/it, loss=0.0326]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/20], Loss: 0.6658, Accuracy: 98.93%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17 / 20: 100%|██████████| 20/20 [02:09<00:00,  6.49s/it, loss=0.0289]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/20], Loss: 0.6048, Accuracy: 99.03%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18 / 20: 100%|██████████| 20/20 [02:15<00:00,  6.76s/it, loss=0.0272]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/20], Loss: 0.5420, Accuracy: 99.17%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19 / 20: 100%|██████████| 20/20 [02:10<00:00,  6.52s/it, loss=0.0232]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/20], Loss: 0.4962, Accuracy: 99.25%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20 / 20: 100%|██████████| 20/20 [02:14<00:00,  6.75s/it, loss=0.0224]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/20], Loss: 0.4445, Accuracy: 99.35%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training metrics from JSON file\n",
        "with open(\"/content/training_metrics.json\", \"r\") as f:\n",
        "    metrics = json.load(f)\n",
        "\n",
        "# Extract loss and accuracy values\n",
        "loss_values = metrics[\"loss\"]\n",
        "accuracy_values = metrics[\"accuracy\"]\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "# Create the plots\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "# Plot loss\n",
        "ax1.set_xlabel(\"Epochs\")\n",
        "ax1.set_ylabel(\"Loss\", color=\"tab:red\")\n",
        "ax1.plot(epochs, loss_values, label=\"Loss\", color=\"tab:red\")\n",
        "ax1.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
        "\n",
        "# Create a second y-axis for accuracy\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel(\"Accuracy (%)\", color=\"tab:blue\")\n",
        "ax2.plot(epochs, accuracy_values, label=\"Accuracy\", color=\"tab:blue\")\n",
        "ax2.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.title(\"Training Loss & Accuracy Over Epochs\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XSq8W81-hfCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model\n",
        "model.eval()\n",
        "\n",
        "# Path to your input raster\n",
        "raster_path = \"/content/s2_d.tif\"\n",
        "output_path = \"/content/predicted_output.tif\"\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Read the raster file\n",
        "with rasterio.open(raster_path) as src:\n",
        "    image = src.read()  # Shape: (bands, height, width)\n",
        "    transform = src.transform\n",
        "    crs = src.crs\n",
        "    profile = src.profile\n",
        "\n",
        "# Get original height and width\n",
        "orig_h, orig_w = image.shape[1], image.shape[2]\n",
        "\n",
        "# Compute padding needed\n",
        "pad_h = (32 - (orig_h % 32)) % 32  # Ensure divisibility by 32\n",
        "pad_w = (32 - (orig_w % 32)) % 32\n",
        "\n",
        "# Pad image (bottom, right)\n",
        "image_padded = np.pad(image, ((0, 0), (0, pad_h), (0, pad_w)), mode='reflect')\n",
        "\n",
        "# Convert to tensor and move to device\n",
        "image_tensor = torch.tensor(image_padded, dtype=torch.float32).unsqueeze(0).to(device)  # (1, bands, H, W)\n",
        "\n",
        "# Run the model\n",
        "with torch.no_grad():\n",
        "    output = model(image_tensor)\n",
        "\n",
        "# Convert predictions to class labels\n",
        "pred_mask_padded = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()  # (H, W)\n",
        "\n",
        "# Remove padding to match original shape\n",
        "pred_mask = pred_mask_padded[:orig_h, :orig_w]\n",
        "\n",
        "# Save the output raster\n",
        "profile.update(dtype=rasterio.uint8, count=1, height=orig_h, width=orig_w, nodata = 0)  # Update metadata\n",
        "\n",
        "with rasterio.open(output_path, \"w\", **profile) as dst:\n",
        "    dst.write(pred_mask.astype(rasterio.uint8), 1)\n",
        "\n",
        "print(f\"Prediction saved at {output_path}\")\n"
      ],
      "metadata": {
        "id": "FIalU2YWhe_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the predicted output\n",
        "output_path = \"/content/predicted_output.tif\"\n",
        "\n",
        "# Read the predicted raster\n",
        "with rasterio.open(output_path) as src:\n",
        "    pred_mask = src.read(1)  # Read the first (and only) band\n",
        "\n",
        "# Plot the predicted mask\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(pred_mask, cmap=\"gray\")  # Use \"gray\" or \"viridis\" for better contrast\n",
        "plt.colorbar(label=\"Class Label\")\n",
        "plt.title(\"Predicted Segmentation Mask\")\n",
        "plt.axis(\"off\")  # Hide axis labels\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D-tVNyS3iRuO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}